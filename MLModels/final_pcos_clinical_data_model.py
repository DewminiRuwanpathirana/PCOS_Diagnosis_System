# -*- coding: utf-8 -*-
"""Final_PCOS_Clinical_data_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jzMw3Mqzq6RSwfrGx0yC2zlntkZOWYdl
"""

# Mount Google Drive to access the dataset
from google.colab import drive
drive.mount('/content/drive')

"""# Importing Libraries"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import roc_curve, auc
import joblib
import random
import shap

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

"""## Load Dataset"""

# Load the datasets
df1 = pd.read_excel("/content/drive/MyDrive/PCOS_Clinical_Data/PCOS_data_without_infertility.xlsx", sheet_name="Full_new")
df2 = pd.read_csv("/content/drive/MyDrive/NEW_PCOS_Clinical_Data/PCOS_data.csv")

# Concatenate the datasets row-wise
df = pd.concat([df1, df2], ignore_index=True)

# Select relevant columns
selected_features = ['PCOS (Y/N)', 'FSH(mIU/mL)', 'LH(mIU/mL)', ' Age (yrs)', 'Weight gain(Y/N)',
                   'hair growth(Y/N)', 'Skin darkening (Y/N)', 'Hair loss(Y/N)', 'Pimples(Y/N)',
                   'Cycle(R/I)', 'Cycle length(days)']
df = df[selected_features]

# Strip extra spaces from column names
df = df.rename(columns=lambda x: x.strip())

# Drop rows with missing values
df = df.dropna()

df.head()

"""## Class Distribution"""

# Plot initial class distribution before SMOTE
plt.figure(figsize=(7, 4))
sns.countplot(x='PCOS (Y/N)', data=df, palette='pastel')
plt.title("Initial Class Distribution Before SMOTE")
plt.show()

# Split the data into features and target
X = df.drop(columns=["PCOS (Y/N)"])
y = df["PCOS (Y/N)"]

# Standardize the features before applying SMOTE
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply SMOTE to the entire dataset before splitting
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Convert resampled data back to a DataFrame and replace df with the balanced dataset
df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),
                pd.Series(y_resampled, name="PCOS (Y/N)")], axis=1)

# Print new class distribution after SMOTE
print("\nClass Distribution After SMOTE:")
print(df["PCOS (Y/N)"].value_counts())

# Plot class distribution after SMOTE
plt.figure(figsize=(7, 4))
sns.countplot(x='PCOS (Y/N)', data=df, palette='pastel')
plt.title("Class Distribution After SMOTE")
plt.show()

# Now split the resampled dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=["PCOS (Y/N)"]), df["PCOS (Y/N)"],
                                                    test_size=0.2, random_state=42)

# Print training distribution
print("\nTraining Set Distribution:")
print(pd.Series(y_train).value_counts())

"""## Correlation Matrix"""

# Calculate the correlation matrix of numeric features
numeric_columns = df.select_dtypes(include='number').columns.tolist()
df_numeric = df[numeric_columns]
corr = df_numeric.corr()

# Set color palette for pastel colors
cmap = sns.color_palette("pastel")

# Create a heatmap for correlation matrix
plt.figure(figsize=(6, 5))
sns.heatmap(corr, annot=True, cmap=cmap, fmt='.2f')
plt.title("Correlation Matrix Heatmap")
plt.show()

"""## Classification"""

# Standardize the features AFTER SMOTE (important to prevent data leakage)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define classifiers (linear and non-linear)
classifiers = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Ridge Classifier": RidgeClassifier(),
    "SVM (Linear)": SVC(kernel='linear', probability=True),
    "SVM (RBF)": SVC(kernel='rbf', probability=True),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)
}

# Evaluate each classifier
best_accuracy = 0
best_model = None
best_model_name = ""

for name, classifier in classifiers.items():
    print(f"\n=== Evaluating {name} ===\n")

    # Train the classifier
    classifier.fit(X_train_scaled, y_train)

    # Predict on the test set
    y_pred = classifier.predict(X_test_scaled)

    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred, output_dict=True)

    # Extract precision, recall, and F1-score for the positive class (PCOS = 1)
    precision = class_report['1']['precision']
    recall = class_report['1']['recall']
    f1 = class_report['1']['f1-score']

    # Print results
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-Score: {f1:.2f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Plot confusion matrix
    plt.figure(figsize=(4, 3))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['No PCOS', 'PCOS'], yticklabels=['No PCOS', 'PCOS'])
    plt.title(f"Confusion Matrix for {name}")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

    # Track the best model based on accuracy
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = classifier
        best_model_name = name

print(f"\nBest Model: {best_model_name} with Accuracy: {best_accuracy:.2f}")

"""## Best Model Evaluation"""

# Store model names and accuracy scores for visualization
model_names = list(classifiers.keys())
accuracy_scores = [accuracy_score(y_test, classifier.predict(X_test_scaled)) for classifier in classifiers.values()]

# Plot classification accuracy for all models
plt.figure(figsize=(8, 5))
plt.barh(model_names, accuracy_scores, color='skyblue')
plt.xlabel("Accuracy Score")
plt.ylabel("Classifier")
plt.title("Classification Accuracy for Different Models")
plt.xlim(0, 1)
plt.gca().invert_yaxis()  # Highest accuracy appears at the top
for i, v in enumerate(accuracy_scores):
    plt.text(v, i, f"{v:.2f}", va='center', fontsize=10)
plt.show()

"""## ROC Curve"""

# Compute predicted probabilities for the best model
y_probs = best_model.predict_proba(X_test_scaled)[:, 1]

# Compute ROC curve and AUC
fpr, tpr, _ = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

# Plot AUC-ROC Curve
plt.figure(figsize=(7, 5))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for reference
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""## Save The Best Model"""

# Save the best model as a joblib file
model_filename = "/content/drive/MyDrive/Saved_Models/best_pcos_clinical_model_1.joblib"
joblib.dump(best_model, model_filename)
print(f"\nBest model '{best_model_name}' saved as '{model_filename}'.")

"""## Test Model Performance"""

# Load the saved model
loaded_model = joblib.load("/content/drive/MyDrive/Saved_Models/best_pcos_clinical_model_1.joblib")

# Select 5 random records from the test set
random_indices = random.sample(range(len(X_test_scaled)), 5)
X_random = X_test_scaled[random_indices]
y_random_actual = y_test.iloc[random_indices]

# Predict using the saved model
y_random_pred = loaded_model.predict(X_random)

# Display results
print("=== Random 5 Predictions from Saved Model ===\n")
for i in range(5):
    print(f"Record {i+1}:")
    print(f"  Actual Label   : {'PCOS' if y_random_actual.iloc[i] == 1 else 'No PCOS'}")
    print(f"  Predicted Label: {'PCOS' if y_random_pred[i] == 1 else 'No PCOS'}\n")

"""## SHAP Explanation"""

# Select a random sample for local explanation
random_index = random.choice(range(len(X_test_scaled)))
random_sample_actual = y_test.iloc[random_index]
random_sample_scaled = X_test_scaled[random_index].reshape(1, -1)
random_sample_features = X_test.iloc[random_index]

# Create a SHAP explainer
# For tree-based models
explainer = shap.TreeExplainer(best_model)

print(f"Created SHAP explainer for {best_model_name}")

# Calculate SHAP values for the random sample
shap_values_random = explainer.shap_values(random_sample_scaled)

# Handle the case where shap_values is a single array or a list of arrays
if isinstance(shap_values_random, list):
    # For multi-class output, use the positive class (index 1)
    if len(shap_values_random) > 1:
        shap_values_flat = shap_values_random[1].flatten()
    else:
        shap_values_flat = shap_values_random[0].flatten()
else:
    # For single output
    shap_values_flat = shap_values_random.flatten()

feature_contributions = pd.DataFrame({
    "Feature": X.columns,  # X.columns has 10 features
    "SHAP Value": shap_values_flat[:len(X.columns)]  # Ensure correct length
})

# Sort by absolute SHAP value to find the most influential features
feature_contributions["Absolute SHAP Value"] = np.abs(feature_contributions["SHAP Value"])
feature_contributions = feature_contributions.sort_values(by="Absolute SHAP Value", ascending=False)

# Print the most influential features for the random sample
print("\nMost Influential Features for the Random Sample (Local Explanation):")
print(feature_contributions[["Feature", "SHAP Value"]].reset_index(drop=True))

# Visualize SHAP Bar Plot for the Random Sample (Local Explanation)
# Plot the most influential features for the random sample
plt.figure(figsize=(10, 6))
sns.barplot(x="SHAP Value", y="Feature", data=feature_contributions, palette="coolwarm")
plt.title("Most Influential Features for the Random Sample (Local Explanation)")
plt.xlabel("SHAP Value (Impact on Prediction)")
plt.ylabel("Feature")
plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)
plt.tight_layout()
plt.show()

print("\nSHAP Bar Plot Explanation for Random Sample:")
print("This plot shows the most influential features for the random sample. "
      "Positive values increase the likelihood of PCOS prediction, while "
      "negative values decrease it.")